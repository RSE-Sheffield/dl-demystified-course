{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7.1 Flowers with data augmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've covered enough to create a model from scratch. Let's apply what we've learned to the `flowers` dataset.\n",
    "\n",
    "First we'll want to import all the packages needed. Notice that this time, layers are imported explicitly so we don't have to keep appending `tf.keras.layers.` when defining our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import layers explicitly to keep our code compact\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset can be downloaded with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_URL = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "\n",
    "zip_file = tf.keras.utils.get_file(origin=_URL,\n",
    "                                   fname=\"flower_photos.tgz\",\n",
    "                                   extract=True)\n",
    "\n",
    "base_dir = os.path.join(os.path.dirname(zip_file), 'flower_photos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we downloaded contains images of 5 types of flowers:\n",
    "\n",
    "1. Rose\n",
    "2. Daisy\n",
    "3. Dandelion\n",
    "4. Sunflowers\n",
    "5. Tulips\n",
    "\n",
    "So, let's create the labels for these 5 classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "classes = ['roses', 'daisy', 'dandelion', 'sunflowers', 'tulips']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the dataset we have downloaded has following directory structure. n\n",
    "<pre style=\"font-size: 10.0pt; font-family: Arial; line-height: 2; letter-spacing: 1.0pt;\" >\n",
    "<b>flower_photos</b>\n",
    "|__ <b>daisy</b>\n",
    "|__ <b>dandelion</b>\n",
    "|__ <b>roses</b>\n",
    "|__ <b>sunflowers</b>\n",
    "|__ <b>tulips</b>\n",
    "</pre>\n",
    "\n",
    "**As you can see there are no folders containing training and validation data. Therefore, we will have to create our own training and validation set. Let's write some code that will do this.**\n",
    "\n",
    "* Create a `train` and a `val` folder each containing 5 folders (one for each type of flower).\n",
    "* Move the images from the original folders to these new folders such that 80% of the images go to the training set and 20% of the images go into the validation set.\n",
    "* In the end our directory will have the following structure:\n",
    "\n",
    "\n",
    "<pre style=\"font-size: 10.0pt; font-family: Arial; line-height: 2; letter-spacing: 1.0pt;\" >\n",
    "<b>flower_photos</b>\n",
    "|__ <b>daisy</b>\n",
    "|__ <b>dandelion</b>\n",
    "|__ <b>roses</b>\n",
    "|__ <b>sunflowers</b>\n",
    "|__ <b>tulips</b>\n",
    "|__ <b>train</b>\n",
    "    |______ <b>daisy</b>: [1.jpg, 2.jpg, 3.jpg ....]\n",
    "    |______ <b>dandelion</b>: [1.jpg, 2.jpg, 3.jpg ....]\n",
    "    |______ <b>roses</b>: [1.jpg, 2.jpg, 3.jpg ....]\n",
    "    |______ <b>sunflowers</b>: [1.jpg, 2.jpg, 3.jpg ....]\n",
    "    |______ <b>tulips</b>: [1.jpg, 2.jpg, 3.jpg ....]\n",
    " |__ <b>val</b>\n",
    "    |______ <b>daisy</b>: [507.jpg, 508.jpg, 509.jpg ....]\n",
    "    |______ <b>dandelion</b>: [719.jpg, 720.jpg, 721.jpg ....]\n",
    "    |______ <b>roses</b>: [514.jpg, 515.jpg, 516.jpg ....]\n",
    "    |______ <b>sunflowers</b>: [560.jpg, 561.jpg, 562.jpg .....]\n",
    "    |______ <b>tulips</b>: [640.jpg, 641.jpg, 642.jpg ....]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO - Build a training and validation dataset as specified above\n",
    "\n",
    "for cl in classes:\n",
    "    img_path = os.path.join(base_dir, cl)\n",
    "    images = glob.glob(img_path + '/*.jpg')\n",
    "    print(\"{}: {} Images\".format(cl, len(images)))\n",
    "    num_train = int(round(len(images)*0.8))\n",
    "    train, val = images[:num_train], images[num_train:]\n",
    " \n",
    "    for t in train:\n",
    "        if not os.path.exists(os.path.join(base_dir, 'train', cl)):\n",
    "            os.makedirs(os.path.join(base_dir, 'train', cl))\n",
    "        shutil.move(t, os.path.join(base_dir, 'train', cl))\n",
    " \n",
    "    for v in val:\n",
    "        if not os.path.exists(os.path.join(base_dir, 'val', cl)):\n",
    "            os.makedirs(os.path.join(base_dir, 'val', cl))\n",
    "        shutil.move(v, os.path.join(base_dir, 'val', cl))\n",
    " \n",
    "# For convenience, let us set up the path for the training and validation sets\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "val_dir = os.path.join(base_dir, 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With the dataset downloaded, we're now ready to build and train our model, you'll need to:**\n",
    "\n",
    "* Create image generators for your trainind and validation dataset, apply all previously mentioned transformation:\n",
    "    * Flip\n",
    "    * Rotation\n",
    "    * Zoom\n",
    "* Define your model\n",
    "* Compile the model\n",
    "* Print out a summary of your model (optional)\n",
    "* Train the model on the created image generators\n",
    "* Plot the output statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO - Create a model as specified above\n",
    "# Setting batch size and image size\n",
    "batch_size = 100\n",
    "IMG_SHAPE = 150 \n",
    " \n",
    "# Create training images generator\n",
    "image_gen_train = ImageDataGenerator(\n",
    "                    rescale=1./255,\n",
    "                    rotation_range=45,\n",
    "                    width_shift_range=.15,\n",
    "                    height_shift_range=.15,\n",
    "                    horizontal_flip=True,\n",
    "                    zoom_range=0.5\n",
    "                    )\n",
    "train_data_gen = image_gen_train.flow_from_directory(\n",
    "                                                batch_size=batch_size,\n",
    "                                                directory=train_dir,\n",
    "                                                shuffle=True,\n",
    "                                                target_size=(IMG_SHAPE,IMG_SHAPE),\n",
    "                                                class_mode='sparse'\n",
    "                                                )\n",
    " \n",
    "# Create validation images generator\n",
    "image_gen_val = ImageDataGenerator(rescale=1./255)\n",
    "val_data_gen = image_gen_val.flow_from_directory(batch_size=batch_size,\n",
    "                                                 directory=val_dir,\n",
    "                                                 target_size=(IMG_SHAPE, IMG_SHAPE),\n",
    "                                                 class_mode='sparse')\n",
    " \n",
    "# Create a CNN model\n",
    "model = Sequential()\n",
    " \n",
    "model.add(Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_SHAPE,IMG_SHAPE, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    " \n",
    "model.add(Conv2D(32, 3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    " \n",
    "model.add(Conv2D(64, 3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    " \n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    " \n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(5))\n",
    " \n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "# Train the model\n",
    "epochs = 5\n",
    " \n",
    "history = model.fit_generator(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=int(np.ceil(train_data_gen.n / float(batch_size))),\n",
    "    epochs=epochs,\n",
    "    validation_data=val_data_gen,\n",
    "    validation_steps=int(np.ceil(val_data_gen.n / float(batch_size)))\n",
    ")\n",
    " \n",
    "# Plot training and validation graphs\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    " \n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    " \n",
    "epochs_range = range(epochs)\n",
    " \n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    " \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7.1 Solution\n",
    "\n",
    "The solution for the exercise can be found [here](https://colab.research.google.com/github/rses-dl-course/rses-dl-course.github.io/blob/master/notebooks/python/solutions/E7.1.ipynb)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Exercise 7.1 Flowers with data augmentation",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
