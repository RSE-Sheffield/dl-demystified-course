# Course Layout
* 00 - Overview
    * Tensorflow Keras
    * Getting setup/Getting course materials
* L00 - Introduction to notebooks
* 01 - Introduction to Deep Learning
    * What is it used for? Who's using it?
        * Examples of applications of DL
        * Specific examples
    * But what is DL?
    * Definitions AI -> ML -> DL
    * ML approaches
        * DL - Neural networks
        * Decision trees
        * SVMs
        * etc.
        * Learning from data
            * Regression example (house prices?, temperature conversion?)
        * How ML is applied 
            * Features
            * Labels
            * Examples
        * Examples
            * House prices
        * Difference in workflow from traditional ML
            * Feature engineering
                * Reducing complex information into yes/no / probability distributions
            * What feature would you make? (House prices, Pictures)
    * Rise of deep learning
        * Hardware, algorithms, data
        * ML/DL algo performance trends
* L01 - TF/Keras & Regression
* 02 - Neural Networks
    * NN history
        * Biological inspiration
    * Terminology
        * Input/Hidden/Output
    * The Dense/fully connected layer 
    * NN equation
        * weights
        * bias
    * How neural networks learn
        * Supervised learning
        * Varying Connection weights, bias
        * Error/Loss function
        * Gradient descent
        * Stochastic gradient descent
        * Back propagation
        * Optimizers
            * Adagrad
            * RMSprop
            * Momentum
            * Adam
        * Losses
            * Mean square error
    * Datasets
        * Training
        * Validation
        * Test
        * Iteration/Epoch
* L02 - Classification with fashion mnist
* 03 - CNNS and classification
    * Classification
        * Logistic regression
    * Losses
        * Cross entropy loss
    * Activations
            * Softmax
            * Sigmoid
            * ReLU     
    * CNN
        * Padding
    * Inputs/Outputs
        * One-hot encoding
    * Feature maps
    * Kernels
    * max pooling
* 04 - Fine-tuning the model
    * Datasets
        * Training
        * Validation
        * Test
        * Iteration/Epoch
    * Stochastic gradient descent
    * Training outputs
        * Visualising the results
        * loss/error
        * accuracy
        * Confusion matrix
    * Regularisation
        * The overfitting problem
        * Early stopping?
        * L2
        * Dropout
        * Use more data
        * Data augmentation
        * Smaller models
    * Hyperparameters
        * Learning rate
        * Network size
        * Batch size
    * Data augmentation
        * Flip
        * Scale
        * Rotate
        * Brightness
        * Colour shifting
        * Noise
* L04a & L04b
* 05 - Transfer learning and saving the model
* L05 & L06
* 06 - Other network types & conclusion
    * Autoencoders    
    * RNNs - LSTM
    * GANs
    * Reinforcement learning
    



